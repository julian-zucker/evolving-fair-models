\documentclass[10pt]{acmart}
\usepackage{palatino,latexsym,natbib,grffile,graphicx,float,hyperref,xcolor}

\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}
\graphicspath{
	{../results/figures/one_metric/}
	{../results/figures/two_metrics/}
}

\newcommand{\fig}[2]{
	\begin{figure}[t]{
		\centering{
			\includegraphics[width=1\textwidth]{#1}
		}
		\caption{#2}
	}
	\end{figure}
}

\newcommand{\figures}[4]{
	\begin{figure}{
		\centering{
			\includegraphics[width=0.74\textwidth]{#1}
		}
		\caption{#2}
		}
	\end{figure}
	\begin{figure} {
		\centering{
			\includegraphics[width=0.74\textwidth]{#3}
		}
		\caption{#4}
	}
	\end{figure}
}


\title{Evolving Fair and Accurate Classification Models}

\author{[Author information elided for peer review]}

\begin{document}

\begin{abstract}
Machine learning models have traditionally been trained to maximize accuracy. Recently, concerns about fairness have motivated the development of multi-objective training methods, which optimize for both accuracy and fairness. Here, we present a multi-objective evolutionary algorithm that produces fair and accurate classification models. We compare our models to the models produced by SciKitLearn, and show that our models have much higher fairness at only a slight cost to accuracy. Furthermore, evolutionary algorithms produce the entire Pareto front, not just one model. Using this Pareto front, we explore the fairness-accuracy tradeoff in a way that would be impossible for many non-evolutionary methods.
\end{abstract}


\maketitle
\section{Introduction}
Machine learning models play a role in decisions that alter the life prospects of their subjects, such as in credit-scoring and recidivism prediction. At the same time, these models are often biased \citep{Angwin:2016}, and because making biased decisions is unethical and illegal \citep{Barocas:2016}, it is important that machine learning practitioners actively reduce bias in their models \citep{Binns:2017}. Bias in algorithmic decision-making has many sources: a model can end up with bias if it is trained on historical data from humans who made biased decisions \citep{Calders:2013}, a misrepresentative subset of data that paints a biased picture \citep{Suresh:2019}, or data with more datapoints for certain subgroups of the population \citep{Torralba:2011}.

Practitioners can identify biases in their models by applying metrics of fairness to their models and datasets. However, it can be difficult for practitioners to choose which metrics to use – the AIF360 project has over 70, none of which are intrinsically better than others \citep{Bellamy:2018}. Many of these metrics are contradictory, that is, an increase in one may necessitate a decrease in another \citep{Kleinberg:2016}. For example, a definition of fairness could measure whether the model’s accuracy is the same for different subgroups. However, this definition of fairness conflicts with accuracy when one subgroup is easier to make predictions for than another. If you maximize accuracy overall, you cannot also have equal accuracy for each subgroup.

Practitioners can mitigate biases by pre-processing input datasets to remove latent bias, post-processing the model’s predictions to ensure the trained model is fair even if the data is biased, or training models using algorithms that take fairness as well as accuracy into account \citep{Angwin:2016}. We propose a new way to train models that are both fair and accurate.

Training a fair and accurate model is a multi-objective optimization problem, because one must optimize simultaneously for accuracy and some definition of fairness. Typical measures of the quality of classification models, such as specificity and AUC, do not capture the fairness in the distribution of benefits to different classes, so fairness requires a second metric, in addition to the metric being used to measure accuracy. We use a multi-objective evolutionary algorithm to train fair and accurate models. First, we show that an evolutionary algorithm, optimizing for accuracy alone, can produce models with similar accuracy to benchmarks. We then show that when optimizing for fairness as well as accuracy, the fair models are only slightly less accurate than the unfair models. The full source code for this analysis is available online \footnote{[Link elided for peer review]}.

\section{Fairness Metrics}
A fairness metric is a function of a model and a dataset that produces a real-valued output representing the degree of unfairness of the given model on the given dataset. For simplicity, we limit our models to produce binary classifications: ``positive outcome'' or ``negative outcome''. We also limit our datasets to only have two groups: ``privileged'' and ``unprivileged.'' Fairness metrics, then, are measures of how fairly the positive and negative outcomes are distributed across the privileged and unprivileged classes \citep{Binns:2017}. We will examine four fairness metrics:

\begin{enumerate}
	\item \textit{Disparate impact,} the ratio between the ratio of positive to negative predictions for the privileged group and the ratio of positive to negative predictions for the unprivileged group. Traditionally, disparate impact is greater than one if the privileged group has a higher disparate impact score, and less than one if the unprivileged group does. We chose to represent the ratio as the higher ratio divided by the lower one, so that the minimum score is 1, and inequality favoring the priviliged or unpriviliged groups causes the score to be greater than 1. This step ensures that minimizing the adjusted score will lead to unbiased models, instead of models biased towards the unpriviliged group.
	\item \textit{False negative rate ratio,} the ratio between the false negative rate for the privileged group and the unprivileged group. The same ratio adjustment as for disparate impact was applied.
	\item \textit{True positive rate ratio,} the ratio between the true positive rate for the privileged group and the unprivileged group. The same ratio adjustment as for disparate impact was applied.
	\item \textit{Between-group generalized entropy,} the amount of information required to represent the inequality in the distribution of benefits between groups by the model. This is the only one of our chosen metrics where a score of 0 represents perfect fairness. We choose specifically the between-group Theil index, a special case of between-group generalized entropy. For a more in-depth description, see \cite{Speicher:2018}.
\end{enumerate}

\section{Evolving Fair Models}
Training a classification model that is both accurate and fair is a multi-objective optimization problem. Because there are multiple objectives, some of which are not differentiable, standard methods of fitting models such as gradient descent and convex optimization---even the multi-objective version of gradient descent proposed by \citep{Desideri:2012}---which rely on a differentiable loss function will not work \citep{Zerbinati:2011}. Furthermore, we are not trying to find the optimal point in Euclidean space by some function, we are trying to find an optimal model. We want to find the optimal model in the space of all possible models, not the optimal point in Euclidean space. Evolutionary optimization approaches have proven effective under these constraints, as in \citep{Zhao:2007} where decision trees were evolved with false negative and false positive rate as objectives. Evolutionary algorithms can also support optimizing models for multiple fairness metrics at once, an advantage over parameterized fairness/accuracy calculations like the ones performed in \citep{Friedler:2019}.

We used the open-source Evvo\footnote{\href{https://github.com/evvo-labs/evvo}{https://github.com/evvo-labs/evvo}} framework to implement the evolutionary algorithm which trains our models. The evolutionary algorithm implemented by Evvo is asynchonous and parallel. It runs as follows:
\begin{enumerate}
	\item Generate a starting population of random solutions.
	\item Starts threads which asynchronously copy and modify solutions in the population. The modified solutions are added to the population.
	\item Start theads which asynchronously select random samples from the population and delete the solutions that are bad. We chose to delete dominated solutions from the sample. Dominated solutions are those which score worse than another solution on every objective. In our case, this means that a model that is both less accurate and less fair than another would be dominated.
	\item When some amount of time has elapsed, stop the system.
\end{enumerate}
To evolve a decision tree with Evvo, one needs an initial population, modification operators, and criteria for when to stop. Our initial population consists of randomly generated full depth-five decision trees, trained by splitting greedily to minimize entropy. We implemented the modification operators introduced in \citep{Kretowski:2005}. These operators change the feature that a node examines, change the threshold of a node, swap the class that a given leaf predicts, change a leaf to a node, or change a node to a leaf. Following \citep{Papagelis:2000}, we employ a “crossover” operator, which takes two decision trees and swaps a random subtree of one with a random subtree of another. After running for a specified amount of time, the system stops and returns the Pareto front.

Machine learning practitioners often evaluate models on accuracy alone, as increasing the accuracy of a model can directly produce value for their employer \citep{Packer:2018}. We show that large increases of fairness are obtainable without sacrificing much accuracy, hopefully laying the groundwork for increased adoption of fair modeling practices.

We evaluated our model training method on four datasets: the German credit dataset, Taiwan credit dataset, and Adult Income dataset from \citep{Dua:2019} and the COMPAS dataset from \citep{Larson:2016}. All four datasets contain numeric attributes and a binary prediction task. For the credit dataset, the task is to predict whether the person described by the data point will default on a loan or not. For the income dataset, the task is to predict whether a person's income is over \$50,000. For the COMPAS dataset, the task is to predict whether a criminal will recidivate. For all of these tasks, there are multiple privileged classes with an intersectional effect. We have chosen to measure bias only across gender in the first three datasets and on race in the COMPAS dataset, for economy of presentation and the simplicity of the resulting fairness metrics. Gender and race, respectively, are not included as inputs to the models being trained.

On each dataset, we will have to define the success of a classification model at the prediction task. To do so, we split each dataset into a training set, consisting of a random sample of 70\% of the data, and a test set, consisting of the remaining 30\% of the data. During evolution, we optimize for accuracy and fairness on the training set. At the end, when we evaluate the model’s performance, we measure its accuracy and fairness on the test set. To quantify how difficult each dataset is to model, we present in Table 1 the accuracy and fairness achieved by some of our evolved decision trees, and compare them to decision trees produced by SciKitLearn \citep{scikit-learn}. Table 1 shows that the models produced by Evvo have worse accuracy than those produced by SciKitLearn, but they are much more fair (as measured by disparate impact).

\renewcommand{\arraystretch}{1.5}
\begin{table}
	\begin{center}
	\begin{tabular}{| l | l | l | l | l |}
	\hline Dataset & Our Acc. & Benchmark Acc. & Our DI & Benchmark DI
	\\ \hline German Credit & .752 & .766 & 1.01 & 1.08
	\\ \hline Taiwan Credit & .817 & .827 & 1.22 & 1.33
	\\ \hline Adult Income  & .804 & .859 & 1.99 & 2.64
	\\ \hline COMPAS        & .663 & .685 & 1.75 & 2.05
	\\ \hline
	\end{tabular}
	\end{center}
    \caption{Accuracy and disparate impact obtained by evolving models using false positive rate and false negative rate as objectives, compared to a SciKitLearn DecisionTreeClassifier trained with a RandomizedSearchCV. Our reported accuracy is the accuracy on the test set of the decision tree with the highest accuracy on the training set. While our models are less accurate than the SciKitLearn models, they are more fair (as measured by disparate impact).}
\end{table}

\section{Results}
Traditional model-training methods only produce one model, while evolutionary algorithms produce a Pareto front. Traditional methods require manual human exploration of the potential models, based on an incomplete understanding of the fairness/accuracy tradeoff space. However, evolutionary algorithms allow the human in charge of developing the model to see the Pareto front of many possible solutions. When examining the Pareto front, it is immediately clear how much an increase in accuracy or fairness would cost in the other metric. The entire search space is presented to the practitioner choosing which model to deploy, allowing for decisions to be made based on more than just a few hand-picked data points.
On the COMPAS dataset, it is clear that improving the fairness of models decreases the model’s accuracy. The points that are in the bottom-left-most parts of Figures 1 and 2, corresponding to the highest accuracy, are the ones with the most unfairness. However, models with near-perfect fairness only slightly underperform the best models. The results were qualitatively similar for the other datasets; in general, fair models only were somewhat less accurate than unfair models.

Disparate impact and true positive rate ratio are both measures of inequality in the allocation of the positive classification. For both measures, an increase in the overall amount of positive predictions allows for a higher value of inequality. Figure 3 shows the opposite effect, because the false negative rate ratio is bounded by the number of total negative predictions.

The value of the overall Theil index must be lower than the maximum entropy for three classes ($\log_2(3) \approx 1.585$) as it is equivalent to the entropy of a series of observations with only three values. And, as noted in \citep{Speicher:2018}, the between-group entropy is a small amount (often less than one percent) of the overall entropy when you have few groups, so the small values in Figure 4 are to be expected.

\figures{Disparate Impact|COMPAS.png}{False Negative Rate, False Positive Rate, and Disparate Impact on the COMPAS dataset. Higher false positive rates are correlated with higher disparate impact scores, and higher-accuracy models have worse disparate impact scores.}{TPR Ratio|COMPAS.png}{False Negative Rate, False Positive Rate, and True Positive Rate Ratio on the COMPAS dataset. As above, the the true positive rate ratio is highest when the false positive rate is highest.}

\figures{FNR Ratio|COMPAS.png}{False Negative Rate, False Positive Rate, and False Negative Rate Ratio on the COMPAS dataset. This figure shows the opposite trend as Figures 1 and 2, as false negative rate ratio is naturally correlated with false negative rate and not false positive rate.}{Between-Group Theil Index|COMPAS.png}{False Negative Rate, False Positive Rate, and Between-Group Theil Index on the COMPAS dataset. Between-Group Theil Index is the only fairness metric that isn't correlated with overall false positive and negative rates.}

Finally, we will examine the maximum accuracy achieved by fair models trained in each of these cases. Table 2 shows the accuracy of our models on the COMPAS dataset, when bounded by some threshold of a fitness metric. The unfair model, trained solely for accuracy, achieved an accuracy of 0.684. There are clearly diminishing returns to decreasing fairness, as most of the accuracy benefits can be obtained at a relatively high level of fairness (namely, 1.1). In fact, a disparate impact of 1.1 falls within the “four-fifths rule” established by Title VII \citep{Barocas:2016}, while having an accuracy only 2\% lower than the highest accuracy of any model we trained. So, ensuring the legality of a model may require only a 2\% decrease in accuracy.

\renewcommand{\arraystretch}{1.5}
\begin{table}
	\begin{center}
	\begin{tabular}{| l | l | l | l |}
	\hline
	Fairness Threshold & TPR Ratio & FNR Ratio & Disparate Impact \\ \hline
	1.00  & 0.580 & 0.633 & 0.539 \\ \hline
	1.05  & 0.653 & 0.660 & 0.568 \\ \hline
	1.10  & 0.653 & 0.660 & 0.634 \\ \hline
	1.20  & 0.653 & 0.660 & 0.634 \\ \hline
	1.30  & 0.653 & 0.660 & 0.634 \\ \hline
	1.40  & 0.654 & 0.660 & 0.634 \\ \hline
	1.50  & 0.658 & 0.660 & 0.635 \\ \hline
	2.00  & 0.661 & 0.662 & 0.653 \\ \hline
	3.00  & 0.661 & 0.667 & 0.657 \\ \hline
	\end{tabular}
	\end{center}
\caption{Peak accuracy for models that achieved different levels of each fairness metric on the COMPAS dataset. Only models with fairness less than or equal to the fairness threshold are included in the accuracy calculation.}
\end{table}


\section{Further Work}
As mentioned when introducing the datasets, each dataset has multiple protected subgroups, and we debiased against only one. Adding objectives for each protected feature may present a different picture. Another straightforward extension would be the application of this framework to different fairness metrics. In addition, the evolution of different types of models could be explored, to see if types other than decision trees may produce fairer, more accurate models. The AIF360 project provides pre- and post-processing steps for models, to reduce their bias. Evolution of models combine with pre- and post-processing might provide an additional way to reduce the cost of fairness.

\section{Conclusion}
We have shown that fair models can be trained with only acceptable losses of accuracy. This result holds across multiple definitions of fairness and multiple datasets. In the face of ethical and legal concerns, this minimal tradeoff may motivate even purely profit-seeking companies to train and deploy fair models. Furthermore, we have used evolutionary computing not only as an optimization technique, but also to produce Pareto front for the exploration of different tradeoffs in fairness. We have explored the three-dimensional Pareto fronts between false negative, false positive, and different fairness metrics, as well as between pairs of fairness metrics, allowing an exploration of the entire accuracy/fairness tradeoff space.

\bibliographystyle{ACM-Reference-Format}
\bibliography{evolving-fair-models.bib}

\end{document}
